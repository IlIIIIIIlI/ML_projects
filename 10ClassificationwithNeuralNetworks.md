https://thecleverprogrammer.com/2022/01/10/classification-with-neural-networks-using-python/

在处理大规模图像数据集的分类任务时，常用的是神经网络技术。以Fashion-MNIST数据集为例，这个数据集包含了70,000个不同的服装图像，目的是通过神经网络训练模型来识别图像所代表的服装类型。

1. **图片分类的挑战性**： 图片分类是一项非常挑战性的任务，特别是当处理大量的图片数据时。传统的机器学习方法很难处理这样的任务，因为图片数据的特征非常丰富和复杂。为了更好地处理图片分类问题，我们通常会使用神经网络。

2. **Fashion-MNIST 数据集**： Fashion-MNIST 是一个包含10种类别的服装图片数据集，其中每张图片都是28x28的灰度图。这个数据集与原始的 MNIST 手写数字数据集非常相似，但它是用于服装分类的。

```Python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
fashion = keras.datasets.fashion_mnist
(xtrain, ytrain), (xtest, ytest) = fashion.load_data()
```

**数据预处理**
为了加快训练，我们需要对图片数据进行归一化，使其值在0到1之间。使用以下方法：

```Python
xvalid, xtrain = xtrain[:5000]/255.0, xtrain[5000:]/255.0
yvalid, ytrain = ytrain[:5000], ytrain[5000:]
```

**神经网络**

```python
# 加载数据
fashion = keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion.load_data()

# 构建模型
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),  # 输入层, 28x28=784 个神经元
    keras.layers.Dense(300, activation="relu"),  # 隐藏层，300个神经元
    keras.layers.Dense(100, activation="relu"),  # 隐藏层，100个神经元
    keras.layers.Dense(10, activation="softmax") # 输出层，10个神经元
])
```

- **神经网络的基本结构**： 一个神经网络通常由三种层组成：输入层、隐藏层和输出层。输入层接收数据，隐藏层对数据进行处理和学习，输出层给出最终的分类结果。
- **Sequential**: 是Keras中用于构建模型的核心类。表示模型是一系列的层。

- **输入层 **Flatten：第一层，负责接收图像数据。在我们的例子中，每个图像大小为28x28像素，因此输入层有784个神经元（28*28）。
- **隐藏层** Dense：一到多个，这些层通过激活函数转换输入数据，进行特征的提取和学习。复杂的问题需要更多的隐藏层。例如，对于含有丰富特征的图像数据，可能需要3-5个隐藏层。在我们的模型中使用了两个，含有300和100个神经元。第一个参数是神经元的数量。`activation`是激活函数，决定神经元的输出。
  - **激活函数**：例如ReLU（修正线性单元）是一种将线性输入转换为非线性输出的函数，有助于捕捉和学习数据中的复杂模式。Softmax用于多分类任务，在输出层将值转换为概率分布
- **输出层**：最后一层，输出最终结果，即分类的类别。在Fashion-MNIST问题中，因为有10种不同的服装，所以有10个神经元。



**模型训练**： 为了训练神经网络，我们需要选择一个适当的损失函数、优化器和评估指标。然后，我们使用训练数据来训练模型，并使用验证数据来评估模型的性能。

```Python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer="sgd",
              metrics=["accuracy"])
history = model.fit(xtrain, ytrain, epochs=30, 
                    validation_data=(xvalid, yvalid))
```

- **损失函数**：`sparse_categorical_crossentropy`，它衡量实际输出与期望输出的差异，指导网络调整权重以最小化损失。这是因为我们的任务是多类别分类

- **优化器**：例如SGD（随机梯度下降），通过调整网络权重以最小化损失函数的算法，使用`accuracy`作为评估指标。

在神经网络中，**参数调优**非常重要。隐藏层的神经元数量、激活函数的选择、训练的轮数（epochs）以及优化器的配置都会影响模型的性能。此外，对于图像分类任务，通常还会引入卷积神经网络（CNN）等更高级的网络结构，因为CNN能更好地处理图像的空间信息，提取有效特征，有助于提高分类的准确性。



**模型预测**

模型训练完成后，我们可以使用以下代码进行预测：

```Python
new = xtest[:5]
predictions = model.predict(new)
classes = np.argmax(predictions, axis=1)
print(classes)
```

**总结**：
对于图片数据，由于其复杂性和维度较高，神经网络比传统机器学习方法更加适用。我们的模型，尽管简单，但已经能够很好地识别Fashion-MNIST中的衣物图片。神经网络通过多层的结构，可以学习并理解数据的深层特征，特别适用于图像、语音、文本等数据密集型任务。

通过激活函数引入的非线性，使得神经网络能适应并解决非线性问题。



**<u>神经网络 NN</u>**

神经网络是模仿人脑神经元工作机制的数学模型，用于处理像图像和语音等复杂输入。一个基本的神经网络由多个层组成：输入层、隐藏层和输出层。每一层有许多的节点，代表神经元，并通过权重（weights）连接。

<u>**数学逻辑**</u>

NN的关键是权重和激活函数。神经元的输入经过加权求和，然后通过激活函数计算，确定神经元是否“激活”或对下一层有贡献。数学上，对于每个神经元，计算如下：
$
output = activation(\sum (input * weight) + bias)
$
其中，$activation()$ 是激活函数，例如ReLU函数（$f(x)=max(0,x)$），用于增加网络对非线性问题的适应性。

神经网络是一个计算模型，灵感来源于人脑的工作原理。它由神经元（或节点）组成，并在各层之间相互连接。

1. **输入层**：网络从数据集接收输入的地方。
2. **隐藏层**：输入和输出之间的层，在此进行计算。
3. **输出层**：网络根据在训练阶段学到的模式对输入数据做出决策的地方。

每个神经元之间的连接都有一个关联的权重，在训练中会进行调整。训练的目标是调整这些权重，使预测输出和实际目标值之间的误差最小化。

<u>**数学原理**</u>

在每个神经元：
1. 将来自上一层的值乘以权重。
2. 这些产品加总，然后加上一个偏置。
3. 此总和然后通过一个激活函数。

数学上，对于单个神经元：

$ z = w_1x_1 + w_2x_2 + ... + w_nx_n + b $
$ a = f(z) $

其中：
- $ x_1, x_2, ... x_n $ 是输入值，
- $ w_1, w_2, ... w_n $ 是权重，
- $ b $ 是偏置，
- $ f $ 是激活函数（如 sigmoid、ReLU 等），
- $ z $ 是加权和，
- $ a $ 是应用激活函数后的输出。

学习过程包括调整权重（和偏置）以最小化预测中的误差。这通常使用反向传播算法和优化方法，如梯度下降来完成。



**<u>MNIST</u>**

MNIST 代表 Modified National Institute of Standards and Technology。它是一个手写数字的数据库，并经常被用作图像分类算法的基准。包含了来自10种类别的服饰图片。

<u>**逻辑**</u>

图片被转化为28x28的像素矩阵，像素值（灰度级）在0-255之间。神经网络通过学习这些像素值与实际标签（0-9的数字或服饰类别）之间的关系，进行分类。

在分类图像时，图像中的每个像素都被用作输入特征。对于 MNIST 中的 28x28 灰度图像，这会导致有 784 个输入特征。像素值范围从0到255，为了获得更好的性能，通常将其归一化到0和1之间。

输出层在 MNIST 的情况下通常会有 10 个神经元（代表数字0-9）。输出层中使用的激活函数通常是 softmax 函数，为每个类别（这里是10个数字）生成一个概率。

$ softmax(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}} $
对于 $ i = 1, ... , K $ 且 $ x $ 是一个向量。

在这里，$ K $ 对于 MNIST 数据集来说是10。

神经网络将调整其权重和偏置，使预测的概率与图像的真实标签匹配。预测的概率与真实标签之间的差异通过损失函数来捕获，分类问题中通常使用的是分类交叉熵：

$ L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i) $

其中：
- $ y $ 是 one-hot 编码形式的真实标签，
- $ \hat{y} $ 是预测的概率分布，
- $ C $ 是类别数量（MNIST 的数量是 10）。

训练神经网络涉及输入网络图像，计算其预测的损失，然后使用反向传播来调整权重和偏置，目标是最小化损失。

这个过程对数据集进行多次迭代（轮次）直到损失收敛到一个最小值或在验证集上开始增加，这表明可能出现了过拟合。



**推导过程**：

1. **前向传播**：计算每层的输出，直到最后的预测结果。以一个简单的3层神经网络为例，其数学表达为：

   - **隐藏层的公式**：
     - $h = activation(X * W_1 + b_1)$

   对应于：

     - $output = activation(\sum (input * weight) + bias)$
     - 和
     - $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$
     - $a = f(z)$

   $X$ 对应于 $input$， $W_1$ 对应于 $weight$, 并且 $b_1$ 对应于 $bias$

   $h$ 是隐藏层的输出，$y$ 是最终的预测

   - **输出层的公式**：
     - $y = softmax(h * W_2 + b_2)$

   在MNIST的上下文中，$y$ 对应于10个神经元的输出，每个神经元对应于一个数字（0-9）

2. **反向传播**：通过误差反向传播，调整权重。首先计算预测输出（$y$）与真实标签（$Y$）之间的误差（损失），例如交叉熵损失函数：

- $L = -\sum{Y * log(y)}$

是：

  - $L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)$

其中，$y$ 是预测的输出，而 $Y$ 是实际的标签。

- 用链式法则计算损失关于权重的梯度，并更新权重以最小化损失：
  - $W_{new} = W_{old} - \alpha * \frac{\partial L}{\partial W}$

其中，$\alpha$ 是学习率，一个超参数。描述了如何基于损失函数 $L$ 的梯度来更新权重 $W$。这是神经网络中通过梯度下降或其他优化算法调整权重的标准方法。



**<u>步骤</u>**

**数据预处理**：

- **加载数据**：从MNIST数据集中加载训练数据和测试数据。
  
- **归一化**：将图像像素值从[0,255]范围转换到[0,1]范围，这有助于提高神经网络的训练效率和性能。
  
- **标签编码**：将标签（例如，数字0-9）转换为one-hot编码格式。

**构建神经网络**：

- **初始化权重和偏置**：为神经网络的每一层随机初始化权重和偏置值。
  
- **选择激活函数**：例如ReLU、sigmoid等。
  
- **定义输出层**：对于MNIST，输出层通常有10个神经元，对应于10个数字类别。

**前向传播**：

- 对于每个输入样本，计算其在网络中的每一层的输出。

**计算损失**：

- 使用预测输出和真实标签计算损失，例如使用交叉熵损失函数。

**反向传播**：

- **计算梯度**：基于损失函数计算网络权重的梯度。
  
- **更新权重和偏置**：使用计算出的梯度和预定的学习率更新网络的权重和偏置。

**迭代**：

- 重复步骤3-5直到满足预定的训练轮数或直到损失不再显著降低。

 **评估模型**：

- 使用独立的测试数据集评估模型的性能。这可以通过计算模型的准确率、查准率、查全率等来完成。

**模型推断**：

- 使用训练好的神经网络模型对新的未知数据进行预测。

