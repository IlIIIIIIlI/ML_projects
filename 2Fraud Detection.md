https://thecleverprogrammer.com/2022/02/22/online-payments-fraud-detection-with-machine-learning/

**1. 数据理解：**

在上述示例中，我们处理的数据集包含了多个特征，如：
- **step**：表示时间的单位（每个步骤等于1小时）。
- **type**：在线交易的类型。
- **amount**：交易金额。
- **nameOrig**、**nameDest**：交易的发起人和接收人。
- **oldbalanceOrg**、**newbalanceOrig**、**oldbalanceDest**、**newbalanceDest**：交易双方在交易前后的账户余额。
- **isFraud**：标记交易是否为欺诈。

**空值与类别探索**

- 确保数据质量通过检测空值。
- 交易类型(`type`)的分布也许能揭示哪种交易更容易成为欺诈的目标。

```Python
print(data.isnull().sum())
print(data.type.value_counts())
```

**相关性分析**

```python
correlation = data.corr()
print(correlation["isFraud"].sort_values(ascending=False))
```

**功能**: 计算数据集中所有特征与“isFraud”（即是否为欺诈交易）的相关性，并按降序打印。

**数据预处理**

进行机器学习任务之前，我们首先要确保数据的质量并选取适当的特征。在原示例中，数据类型（type）被映射为整数，这是因为大多数机器学习算法需要数值输入。此外，我们也将目标变量'isFraud'映射为可读的字符串，这在实际中可能不是必要的。

```python
data["type"] = data["type"].map({"CASH_OUT": 1, "PAYMENT": 2, 
                                 "CASH_IN": 3, "TRANSFER": 4,
                                 "DEBIT": 5})
data["isFraud"] = data["isFraud"].map({0: "No Fraud", 1: "Fraud"})
print(data.head())
```

**功能**: 这部分代码将交易类型和欺诈标签从文字转化为可以用于模型训练的数值或分类标签。

**模型评估**

模型的性能通过多种指标进行评估。在此示例中，我们使用的是准确性，但在不平衡数据集（如欺诈检测）中，其他指标（如精准率、召回率或F1分数）可能更有相关性。

```Python
print(model.score(xtest, ytest))
```

---

**模型分析**

```Python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
x = np.array(data[["type", "amount", "oldbalanceOrg", "newbalanceOrig"]])
y = np.array(data[["isFraud"]])
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.10, random_state=42)
model = DecisionTreeClassifier()
model.fit(xtrain, ytrain)
```

决策树模型是监督学习算法中的一种，它通过从数据特征中学习决策规则来做预测。决策树以树形结构呈现，每个节点表示一个属性或特征，每个分支代表一个决策规则，而每个叶节点代表一个输出。在欺诈检测的问题中，数据通常是高度不平衡的，因为在大量合法交易中，欺诈行为通常很少见。决策树因其能够很好处理不平衡数据集和非线性特征关系而成为这类问题的一个合适选择。另外，决策树能够提供清晰的决策路径和规则，这在解释模型预测与做决策时尤为重要。

1. **易于理解和解释：**
   - 决策树的结果易于理解，无需复杂的统计知识即可解释模型逻辑。
   - 通过图形化工具，可以直观地展示决策路径，方便与非技术专家共享理解。

2. **需要较少的数据预处理：**
   - 不需要归一化或标准化数据。
   - 由于决策树可以处理非线性特征关系和不平衡数据集，因此它们通常用于分类问题，比如欺诈检测。
   
3. **能够处理多种数据类型：**
   - 能够同时处理数值和分类数据。
   - 可以处理缺失值。
   
4. **特征重要性：**
   - 可以直观地展示各个特征对预测目标的重要性。
   
5. **适用于非线性问题：**
   - 能够很好地拟合非线性关系。

<u>局限性</u>

1. **过拟合问题：**
   - 决策树很容易生成复杂的树结构，紧密地拟合数据，从而捕获到数据中的噪声。
   
2. **稳定性较低：**
   - 数据的小变化可能导致生成一个完全不同的树。

3. **局部最优问题：**
   - 通过贪婪算法优化，可能导致在全局范围内并非最优解。

---

**决策树需要知道的知识 - 信息熵的推导**

信息熵是度量数据中信息量的一个衡量标准，它表达的是数据的混乱程度。对于给定的数据集 $D$，其信息熵 $H(D)$ 的计算方法为：

$ H(D) = -\sum_{k=1}^{m} p_k \log_2 p_k $

这里的 $p_k$ 是数据集 $D$ 中第 $k$ 类样本所占的比例。

**为什么要这样定义信息熵呢？**

熵起源于热力学作为体系混乱度的度量，在信息论中引入了信息熵的概念来度量信息的不确定性。如果所有的消息都是等概率的，那么熵最大，不确定性也最大。为了得到这个表达式，基于一些公理化的标准，如非负性、可加性等，最终可以得到上述熵的定义表达式。

<u>**决策树工作原理**</u>

决策树通过优化一个或多个评判准则来选择分裂节点的特征和划分阈值，从而构建一棵分裂决策的树。目标是生成一个结构简单（例如，树的深度较小）、但预测能力强的决策树。为达成这个目标，我们需要一些数学方法来量化特征选择的"优良"程度。

> **节点分裂的依据**

**信息增益 (Information Gain, IG)**

信息增益用来度量通过分裂节点来减少不确定性（或信息熵）的程度。如果分裂后的数据子集的不确定性（或混乱度）下降，我们认为这是一个好的分裂。选择信息增益最大的特征进行分裂。

给定数据集 $D$ 和一个特征 $A$，我们希望通过计算信息增益来判断使用特征 $A$ 对数据集 $D$ 进行分裂的优良性。信息增益 $IG(D, A)$ 定义为数据集 $D$ 的熵 $H(D)$ 与在特征 $A$ 给定条件下 $D$ 的条件熵 $H(D|A)$ 之差：

$ IG(D, A) = H(D) - H(D|A) $

where

$ H(D) = -\sum_{k=1}^{m} p_k \log_2 p_k $

其中$p_k$是选择第k类的概率。

$ H(D|A) = \sum_{v \in Values(A)} \frac{|D_v|}{|D|} H(D_v) $

其中$H(D)$是数据集$D$的熵，$Values(A)$是特征$A$的所有可能值，$D_v$ 是 $A$ 取值为 $v$ 的所有样本的子集，$|D_v|/|D|$是其占比。

条件熵 $H(D|A)$ 表示在已知特征 $A$ 的情况下 $D$ 的不确定性。我们希望在特征 $A$ 给定的条件下，数据集 $D$ 的不确定性尽可能地小，这样我们就能通过观察特征 $A$ 的取值来获取尽可能多的关于 $D$ 的信息。

**基尼不纯度 (Gini Impurity)**

基尼不纯度用来衡量一个数据集的混乱程度。基尼不纯度小说明数据子集的纯度较高，即类别较为单一。选择基尼不纯度最小的特征进行分裂。

基尼不纯度源自经济学中的基尼系数，用于度量一个经济体中的贫富不均。在决策树中，我们使用基尼不纯度来度量数据的纯度：

$ Gini(D, A) = \sum_{v \in Values(A)} \frac{|D_v|}{|D|} (1 - \sum_{k=1}^{m}(p_{k|A=v})^2) $

这里，$p_{k|A=v}$ 是在 $A$ 取值为 $v$ 时选择第 $k$ 类的概率。

<u>为什么要用这个形式的基尼不纯度呢？</u>

基尼不纯度度量的是数据集中的样本被选中的概率和被错误分类的概率的乘积。我们希望这个值尽可能小，即数据子集应该尽可能地"纯"，也就是说，一个子集中不同类别的样本所占的比例应该尽可能地倾斜。如果一个子集中的所有样本都属于同一类别，那么基尼不纯度就为0，这是最理想的情况。

>**停止分裂的条件**

- 所有实例都属于同一类别；
- 所有特征取值相同；
- 分裂无法提高预测精度/减小不纯度。

```python
from sklearn.tree import DecisionTreeClassifier

# 使用基尼不纯度
clf_gini = DecisionTreeClassifier(criterion='gini')

# 使用信息增益
clf_entropy = DecisionTreeClassifier(criterion='entropy')
```

在`scikit-learn`的决策树实现中，这些方法旨在减少分裂后数据的不确定性（或者增加数据的纯度）。在构建树的过程中，算法会遍历所有可能的分裂点，并选择使得目标准则最小化（或最大化）的分裂点。在计算了每个特征的不纯度或信息增益后，算法会选择“最好”的特征进行分裂，并递归地重复这个过程，直到树达到预定的最大深度或者无法进一步减少节点的不纯度为止。